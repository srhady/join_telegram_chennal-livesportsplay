# প্রয়োজনীয় লাইব্রেরি ইম্পোর্ট করা হচ্ছে
import requests
from bs4 import BeautifulSoup
import datetime
from urllib.parse import urljoin, quote
import json

# টার্গেট ওয়েবসাইটের URL
WEBSITE_URL = "https://bingsport.watch/" 

# প্লেলিস্ট ফাইলের নাম
PLAYLIST_FILE = "playlist.m3u"

# হেডারগুলো এখানে সংজ্ঞায়িত করা হয়েছে
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'
REFERER = WEBSITE_URL

def get_stream_links():
    """
    এই ফাংশনটি bingsport.watch থেকে স্ট্রিম লিঙ্ক সংগ্রহ করে।
    এটি ওয়েবসাইটের内部 API ব্যবহার করে সরাসরি লাইভ ম্যাচের তালিকা সংগ্রহ করে।
    """
    stream_links = set()
    
    # --- চূড়ান্ত পদ্ধতি: ওয়েবসাইটের API ব্যবহার করা ---
    # এই ওয়েবসাইটটি একটি API এন্ডপয়েন্ট থেকে লাইভ ম্যাচের ডেটা লোড করে।
    # আমরা সরাসরি সেই API-তে অনুরোধ পাঠাব।
    api_url = "https://bingsport.watch/api/matches/live"
    
    headers = {
        'User-Agent': USER_AGENT,
        'Referer': WEBSITE_URL,
        'X-Requested-With': 'XMLHttpRequest' # এটি একটি AJAX অনুরোধ হিসেবে চিহ্নিত করে
    }

    try:
        print(f"Fetching live match data from API: {api_url}")
        response = requests.get(api_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # API থেকে পাওয়া ডেটা JSON ফরম্যাটে থাকে
        data = response.json()
        
        # JSON ডেটা থেকে ম্যাচের লিঙ্কগুলো বের করা হচ্ছে
        if not data.get('matches'):
            print("API response does not contain 'matches' key or it's empty.")
            return []

        for match in data['matches']:
            # প্রতিটি ম্যাচের জন্য একটি URL থাকে
            match_slug = match.get('url')
            if match_slug:
                match_link = urljoin(WEBSITE_URL, match_slug)
                print(f"-> Found match page from API: {match_link}")
                
                # এখন আমরা ম্যাচের পেইজে গিয়ে iframe লিঙ্ক খুঁজব
                try:
                    match_page_response = requests.get(match_link, headers=headers, timeout=30)
                    match_page_soup = BeautifulSoup(match_page_response.content, "html.parser")
                    
                    iframe = match_page_soup.find('iframe')
                    if iframe and iframe.get('src'):
                        stream_src = iframe['src']
                        # কিছু ক্ষেত্রে src লিঙ্ক '//' দিয়ে শুরু হয়
                        if stream_src.startswith('//'):
                            stream_src = 'https:' + stream_src
                            
                        full_stream_link = urljoin(WEBSITE_URL, stream_src)
                        stream_links.add(full_stream_link)
                        print(f"   Found stream link: {full_stream_link}")
                except Exception as e:
                    print(f"   Could not process match page {match_link}. Error: {e}")
                    continue
        
        print(f"\nTotal unique stream links found: {len(stream_links)}")
        return list(stream_links)

    except Exception as e:
        print(f"An error occurred during API scraping: {e}")
        return []

def create_playlist(links):
    """
    এই ফাংশনটি লিঙ্কগুলো ব্যবহার করে একটি .m3u প্লেলিস্ট ফাইল তৈরি করে।
    """
    if not links:
        with open(PLAYLIST_FILE, "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n# No live streams found at the moment.\n")
        return

    with open(PLAYLIST_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        f.write(f"# Autogenerated on: {datetime.datetime.now().isoformat()}\n")
        f.write(f"# Total Streams: {len(links)}\n\n")
        
        for i, link in enumerate(links):
            try:
                stream_name = link.split('/')[2].replace('www.', '')
            except:
                stream_name = f"Stream {i+1}"
            
            encoded_user_agent = quote(USER_AGENT)
            formatted_link = f"{link}|User-Agent={encoded_user_agent}&Referer={REFERER}"
            
            f.write(f"#EXTINF:-1 tvg-id=\"\" tvg-name=\"{stream_name}\" group-title=\"Live\",{stream_name}\n")
            f.write(f"{formatted_link}\n")
    
    print(f"Playlist '{PLAYLIST_FILE}' was updated successfully with {len(links)} streams.")

if __name__ == "__main__":
    print("Starting final API-based scraper...")
    final_links = get_stream_links()
    create_playlist(final_links)
    print("Scraping process finished.")

