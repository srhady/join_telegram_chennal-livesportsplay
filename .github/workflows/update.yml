name: Auto Update TV Links

on:
  schedule:
    - cron: '*/15 * * * *'  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡ßß‡ß´ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
  workflow_dispatch:  # ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡ßü‡¶æ‡¶≤‡¶ø ‡¶∞‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶Ö‡¶™‡¶∂‡¶®

permissions:
  contents: write  # ‡¶∞‡¶ø‡¶™‡ßã‡¶§‡ßá ‡¶≤‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶™‡¶æ‡¶∞‡¶Æ‡¶ø‡¶∂‡¶®

jobs:
  update-links:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0  # ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ó‡¶ø‡¶ü ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø ‡¶´‡ßá‡¶ö ‡¶ï‡¶∞‡ßá

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install requests beautifulsoup4

    - name: Scrape and update links
      run: |
        cat << 'EOF' > scraper.py
        import requests
        from bs4 import BeautifulSoup
        from datetime import datetime
        import os

        url = "https://www.bdixtv24.xyz"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        try:
            print("üîç Fetching data from:", url)
            response = requests.get(url, headers=headers, timeout=20)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            links = []
            
            print("üîó Finding streaming links...")
            # Adjust this selector based on actual website structure
            for a in soup.find_all("a", href=True):
                href = a["href"].lower()
                if "stream" in href or "live" in href:
                    full_url = a["href"] if a["href"].startswith("http") else f"{url}{a['href']}"
                    title = a.text.strip() or f"Channel {len(links)+1}"
                    links.append(f"- [{title}]({full_url})")
                    print(f"‚úÖ Found: {title} - {full_url}")

            if not links:
                print("‚ö†Ô∏è No streaming links found!")
                links.append("- No live streams available at the moment")

            with open("TV_Links.md", "w", encoding="utf-8") as f:
                f.write("# üì∫ ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶ü‡¶ø‡¶≠‡¶ø ‡¶≤‡¶ø‡¶Ç‡¶ï‡¶∏\n\n")
                f.write(f"**‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü:** {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n\n")
                f.write("\n".join(links))
                f.write("\n\n---\n\n> ‡¶∏‡ßç‡¶¨‡ßü‡¶Ç‡¶ï‡ßç‡¶∞‡¶ø‡ßü‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá")
            
            print("üìÑ TV_Links.md file updated successfully!")

        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            with open("TV_Links.md", "w", encoding="utf-8") as f:
                f.write("# ‚ö†Ô∏è ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø\n\n")
                f.write(f"‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n\n")
                f.write(f"‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø‡¶∞ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§:\n```\n{str(e)}\n```")
        EOF

        python scraper.py

    - name: Commit and push changes
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add TV_Links.md
        if ! git diff --cached --quiet; then
          git commit -m "Auto-update TV links [$(date +'%d-%m-%Y %H:%M')]"
          git push
        else
          echo "No changes to commit."
        fi
