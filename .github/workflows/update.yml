name: Auto Update TV Links

on:
  schedule:
    - cron: '*/15 * * * *'  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡ßß‡ß´ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
  workflow_dispatch:  # ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡ßü‡¶æ‡¶≤ ‡¶∞‡¶æ‡¶® ‡¶Ö‡¶™‡¶∂‡¶®

permissions:
  contents: write  # ‡¶∞‡¶ø‡¶™‡ßã‡¶§‡ßá ‡¶≤‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶™‡¶æ‡¶∞‡¶Æ‡¶ø‡¶∂‡¶®

jobs:
  update-links:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install requests beautifulsoup4

    - name: Scrape and update links
      run: |
        cat << 'EOF' > scraper.py
        import requests
        from bs4 import BeautifulSoup
        from datetime import datetime

        url = "https://www.bdixtv24.xyz"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        try:
            print("Fetching data from:", url)
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            links = []
            
            print("Finding links...")
            for a in soup.find_all("a", href=True):
                if "stream" in a["href"].lower():
                    link = a["href"] if a["href"].startswith("http") else f"{url}{a['href']}"
                    title = a.text.strip() or f"Channel {len(links)+1}"
                    links.append(f"- [{title}]({link})")
                    print(f"Found: {title} - {link}")

            if not links:
                print("No streaming links found!")
                links.append("- No live streams available currently")

            with open("TV_Links.md", "w", encoding="utf-8") as f:
                f.write("# üì∫ ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶ü‡¶ø‡¶≠‡¶ø ‡¶≤‡¶ø‡¶Ç‡¶ï‡¶∏\n\n")
                f.write(f"**‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü:** {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n\n")
                f.write("\n".join(links))
                f.write("\n\n---\n\n> ‡¶∏‡ßç‡¶¨‡ßü‡¶Ç‡¶ï‡ßç‡¶∞‡¶ø‡ßü‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá")
                
            print("File updated successfully!")

        except Exception as e:
            print(f"üö® Error: {e}")
            with open("TV_Links.md", "w", encoding="utf-8") as f:
                f.write("# ‚ö†Ô∏è ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø\n\n")
                f.write(f"‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n")
                f.write(f"```\n{e}\n```")
        EOF

        python scraper.py

    - name: Commit changes
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add TV_Links.md
        git commit -m "Auto-update TV links [$(date +'%d-%m-%Y %H:%M')]"
        git push
